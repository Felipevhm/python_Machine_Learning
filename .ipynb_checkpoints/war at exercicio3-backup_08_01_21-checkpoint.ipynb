{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidade Federal de Santa Catarina<br>\n",
    "Departamento de Engenharia Elétrica e Eletrônica<br>\n",
    "EEL7514/EEL7513/EEL410250 - Aprendizado de Máquina\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\bzero}{\\mathbf{0}}$\n",
    "\n",
    "\n",
    "# Exercício 3: Regressão Linear & Otimização Numérica\n",
    "\n",
    "Neste exercício você irá explorar métodos de otimização numérica para treinar um modelo de regressão linear. Em particular, você irá implementar o método do gradiente e analisar sua convergência. Além disso, você irá investigar o efeito da normalização de atributos no comportamento do método. Finalmente, você irá investigar a aplicação de regressão linear em um conjunto de dados real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto de dados #1\n",
    "\n",
    "Inicialmente, utilizaremos o mesmo conjunto de dados do exercício anterior, exceto por uma escala diferente em $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def gen_data(n_samples, x_scale=[0,1], noise=0.5):\n",
    "    '''Generate univariate regression dataset'''\n",
    "    x = np.sort(np.random.rand(n_samples))\n",
    "    y = 6*(-1/6 + x + (x > 1/3)*(2/3-2*x) + (x > 2/3)*(2*x-4/3)) + noise*np.random.randn(n_samples)\n",
    "    x = x_scale[0] + (x_scale[1]-x_scale[0])*x\n",
    "    X = x.reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "def plot_data(X, y):\n",
    "    '''Plot univariate regression dataset'''\n",
    "    assert len(X.shape) == 2 and len(y.shape) == 1\n",
    "    plt.plot(X[:,0],y,'b.'); plt.xlabel('x'); plt.ylabel('y');\n",
    "    return\n",
    "\n",
    "def plot_prediction(model, X, y, n_points=100):\n",
    "    '''Plot dataset and predictions for a univariate regression model'''\n",
    "    plot_data(X,y)\n",
    "    if n_points is not None:\n",
    "        xx = np.linspace(X.min(),X.max(),n_points)\n",
    "        yy = model.predict(xx.reshape(-1,1))\n",
    "        plt.plot(xx,yy,'r-')\n",
    "    y_pred = model.predict(X)\n",
    "    plt.plot(X[:,0],y_pred,'r.')\n",
    "    plt.legend(['True', 'Predicted'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados pode ser gerado e visualizado pelos comandos abaixo (observe a nova escala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1) (30,)\n",
      "(1000, 1) (1000,)\n",
      "(1000, 1) (1000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(2019*2)\n",
    "X, y = gen_data(n_samples=30, x_scale=[0,4])\n",
    "X_val, y_val = gen_data(n_samples=1000, x_scale=[0,4])\n",
    "X_test, y_test = gen_data(n_samples=1000, x_scale=[0,4])\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Plot only the training data!\n",
    "plot_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Método do gradiente\n",
    "\n",
    "Resgate a classe do modelo que você implementou no exercício anterior. Iremos reorganizá-la para permitir um método de treinamento alternativo.\n",
    "\n",
    "1. Utilize a classe abaixo, substituindo na função `_fit_ne` a sua função `fit` implementada anteriormente, com as modificações necessárias. Note que a função `add_powers` foi eliminada (bem como o argumento `d` da inicialização do modelo), sendo substituída pela função `_add_ones` (que simplesmente adiciona uma coluna de 1's). Ou seja, nosso modelo deve implementar puramente uma regressão linear (com regularização L2), sem atributos adicionais. Caso desejemos atributos polinomiais, poderemos usar a classe `PolynomialFeatures` do sklearn. A única vantagem do nosso modelo de regressão próprio em relação ao `Ridge` é permitir utilizar um método de treinamento diferente.\n",
    "1. Mova a função `mse` para fora da classe, caso contrário não poderemos acessá-la dentro de um `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    # Linear regression with L2 regularization\n",
    "    def __init__(self, lamb=0, solver='ne', lr=1, maxiter=1000, tol=1e-5,dg=1):\n",
    "        # Initialization\n",
    "        self.lamb = lamb\n",
    "        self.solver = solver\n",
    "        self.lr = lr\n",
    "        self.maxiter = maxiter\n",
    "        self.tol = tol\n",
    "        self.dg=dg\n",
    "        return\n",
    "    \n",
    "    def _add_ones(self, X):\n",
    "        # Add column of ones\n",
    "        X_new = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X_new\n",
    "# 3.1.3 -------------------------------------------------\n",
    "    def _fit_ne(self, X, y): \n",
    "        d=X.shape[1]\n",
    "        X = self._add_ones(X)\n",
    "        L = np.zeros([d+1,d+1])\n",
    "        for i in range(1,d+1):\n",
    "            L[i,i]=1\n",
    "        \n",
    "        self.hessian = hessian(X)\n",
    "        print (\"np.linalg.cond(self.hessian) == \\n \",np.linalg.cond(self.hessian),\"\\n\")\n",
    "        lamb = self.lamb\n",
    "        assert np.linalg.matrix_rank(X.T @ X + lamb*L) == X.shape[1], 'Singular matrix'\n",
    "        self.w = np.linalg.inv(np.transpose(X)@X+lamb*L)@np.transpose(X)@y\n",
    "        return\n",
    "\n",
    "# 3.1.4 -------------------------------------------------\n",
    "    def _fit_gd(self, X, y,det=False): \n",
    "        # Fit by gradient descent\n",
    "        \n",
    "        X=self._add_ones(X)\n",
    "        w=np.zeros([X.shape[1],1])\n",
    "        print(w.shape)\n",
    "        self.w=w\n",
    "        print(X.shape[1])\n",
    "        lr = self.lr*(0.1**(X.shape[1]-1))\n",
    "       # print(\"LR  {}\".format(lr))\n",
    "        maxiter = self.maxiter\n",
    "        tol = self.tol\n",
    "        self.J_history = []\n",
    "        J_history =self.J_history\n",
    "        \n",
    "        #\n",
    "       # print(\"maxiter == \",maxiter)\n",
    "        #print(\"\\nrange(maxiter) == \",range(maxiter))\n",
    "        #print(\"\\nJ_history == \",J_history)\n",
    "        \n",
    "        for it in range(maxiter):\n",
    "           # print(\"IT == \",it)\n",
    "            grad_J = self.grad_J(X,y)\n",
    "\n",
    "            J_history.append(np.linalg.norm(self.J(X,y)))\n",
    "            w -= lr*grad_J\n",
    "            self.w = w\n",
    "            if det:\n",
    "                if (it+1)%(maxiter/10) == 0:\n",
    "                    print(\"\\n# de iterações ==    {}\".format(it+1),\"\\n\\n\",\n",
    "                         \"\\nw == {}\".format(w),\"\\n\\n\",\n",
    "                         \"\\ngradJ == {}\".format(grad_J),\"\\n\\n\",\n",
    "                         \"\\n|gradJ| == {:.6f}\".format(J_history[-1]),\"\\n\\n\",\n",
    "                         \"\\n#J == {:.6f}\".format(self.J(X,y)),\"\\n\\n\",\n",
    "                         \"\\n---------------\")\n",
    "            \n",
    "            if np.linalg.norm(grad_J) <tol:   \n",
    "                 print(\"\\n---------------\",\n",
    "                         \"\\nUnder tolerance\",\"\\n\\n\",\n",
    "                         \"\\n # de iterações == {}\".format(it+1),\"\\n\\n\",\n",
    "                         \"\\nw == {}\".format(w),\"\\n\\n\",\n",
    "                         \"\\ngradJ == {}\".format(grad_J),\"\\n\\n\",\n",
    "                         \"\\n|gradJ| == {:.6f}\".format(J_history[-1]),\"\\n\\n\",\n",
    "                         \"\\n#J == {:.6f}\".format(self.J(X,y)),\"\\n\\n\",\n",
    "                         \n",
    "                         \"\\n---------------\"); break\n",
    "            elif it == maxiter -1:\n",
    "                print(\"Reached maxiter\")\n",
    "            self.J_history = J_history\n",
    "            self.w=w\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y,det=False):\n",
    "        if self.solver == 'gd':\n",
    "            self._fit_gd(X, y,det)\n",
    "        elif self.solver == 'ne':\n",
    "            self._fit_ne(X, y)\n",
    "        else:\n",
    "            raise RuntimeError('Unknown solver')\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        w=self.w\n",
    "        y_pred = X@w\n",
    "        return y_pred\n",
    "    \n",
    "    def reg_J(self,X,y):\n",
    "        m=1/(X.shape[0])\n",
    "        w=self.w\n",
    "        xawmy = ((X@w).T - y.T)\n",
    "        return (m)*(np.linalg.norm(xawmy)**2+ self.lamb *np.linalg.norm(w.T)**2)\n",
    "    \n",
    "    def J(self,X,y):\n",
    "        m=1/(X.shape[0])\n",
    "        w=self.w\n",
    "        xawmy = ((X@w).T - y.T).T\n",
    "        return (m)*(np.linalg.norm(xawmy)**2)\n",
    "    \n",
    "    def grad_J(self,X,y):\n",
    "        m=1/(X.shape[0])\n",
    "        w=self.w\n",
    "        xawmy = ((X@w).T - y.T).T\n",
    "        return (2*m)*(X.T@xawmy+self.lamb*w)\n",
    "    \n",
    "    def hess_J(self,X,y):\n",
    "        m = 1/(X.shape[0])\n",
    "        lamb=self.lamb\n",
    "        d= X.shape[0]\n",
    "        I = np.identity(d)\n",
    "        return (2*m)*(X.T@X-lamb*I)\n",
    "    \n",
    "# 3.1.2 -------------------------------------------------\n",
    "def mse(model, X, y):\n",
    "    m=X.shape[0]\n",
    "    X = model._add_ones(X)\n",
    "    y_pred = model.predict(X)\n",
    "    y= np.reshape(y,-1)\n",
    "    y_pred = np.reshape(y_pred,-1)\n",
    "    J = np.sum((y_pred-y)**2)/m\n",
    "    return J\n",
    "\n",
    "def hessian(x):\n",
    "    x_grad = np.gradient(x)\n",
    "    hessian = np.empty((x.ndim,x.ndim)+x.shape,dtype=x.dtype)\n",
    "    for k,grad_k in enumerate(x_grad):\n",
    "        tmp_grad = np.gradient(grad_k)\n",
    "        for l, grad_kl in enumerate (tmp_grad):\n",
    "            hessian[k,l,:,:] = grad_kl\n",
    "    return hessian\n",
    "\n",
    "def grad(x):\n",
    "    return np.gradient(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.linalg.cond(self.hessian) == \n",
      "  [[           inf 2.34655329e+16]\n",
      " [2.34655329e+16            inf]] \n",
      "\n",
      "w = [-0.20189161  0.02333163] \n",
      "Train MSE: 0.517264 \n",
      "Val MSE: 0.601059\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.fit(X,y)\n",
    "print('w =',model.w,'\\nTrain MSE: %f'% mse(model,X,y),'\\nVal MSE: %f'% mse(model,X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- LR ==  1 ------------------\n",
      "\n",
      "\n",
      "Solver ==  gd \n",
      "\n",
      "(2, 1)\n",
      "2\n",
      "\n",
      "--------------- \n",
      "Under tolerance \n",
      "\n",
      " \n",
      " # de iterações == 197 \n",
      "\n",
      " \n",
      "w == [[-0.2018722 ]\n",
      " [ 0.02332372]] \n",
      "\n",
      " \n",
      "gradJ == [[ 9.21825763e-06]\n",
      " [-3.75539546e-06]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.517264 \n",
      "\n",
      " \n",
      "#J == 0.517264 \n",
      "\n",
      " \n",
      "---------------\n",
      "w = [[-0.2018722 ]\n",
      " [ 0.02332372]] \n",
      "\n",
      "Train MSE: 0.517264 \n",
      "Val MSE: 0.601058\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = Model(solver= 'gd',lr=1,maxiter = 10000, dg=1)\n",
    "#print (\"---------------- LR == \",model.lr,\"------------------\\n\")\n",
    "\n",
    "print (\"---------------- LR == \",model.lr,\"------------------\\n\")\n",
    "m_solver=model.solver\n",
    "\n",
    "print(\"\\nSolver == \",m_solver,\"\\n\")\n",
    "model.fit(X,y,det=True)\n",
    "#_fit_gd(model,X,y)\n",
    "print('w =',model.w,'\\n\\nTrain MSE: %f'% mse(model,X,y),'\\nVal MSE: %f'% mse(model,X_val,y_val))\n",
    "\n",
    "if m_solver == 'gd':\n",
    "    plt.plot(model.J_history)\n",
    "    plt.show\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- LR ==  0.1 ------------------\n",
      "\n",
      "\n",
      "Solver ==  gd \n",
      "\n",
      "(2, 1)\n",
      "2\n",
      "\n",
      "# de iterações ==    1000 \n",
      "\n",
      " \n",
      "w == [[-0.19996622]\n",
      " [ 0.02254725]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.00087709]\n",
      " [-0.00035732]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.517265 \n",
      "\n",
      " \n",
      "#J == 0.517265 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    2000 \n",
      "\n",
      " \n",
      "w == [[-0.20187116]\n",
      " [ 0.0233233 ]] \n",
      "\n",
      " \n",
      "gradJ == [[ 9.31428280e-06]\n",
      " [-3.79451484e-06]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.517264 \n",
      "\n",
      " \n",
      "#J == 0.517264 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "--------------- \n",
      "Under tolerance \n",
      "\n",
      " \n",
      " # de iterações == 2002 \n",
      "\n",
      " \n",
      "w == [[-0.20187135]\n",
      " [ 0.02332337]] \n",
      "\n",
      " \n",
      "gradJ == [[ 9.22999848e-06]\n",
      " [-3.76017852e-06]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.517264 \n",
      "\n",
      " \n",
      "#J == 0.517264 \n",
      "\n",
      " \n",
      "---------------\n",
      "w = [[-0.20187135]\n",
      " [ 0.02332337]] \n",
      "\n",
      "Train MSE: 0.517264 \n",
      "Val MSE: 0.601058\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "\n",
    "model = Model(solver= 'gd',lr=0.1,maxiter = 10000, dg=1)\n",
    "#print (\"---------------- LR == \",model.lr,\"------------------\\n\")\n",
    "\n",
    "print (\"---------------- LR == \",model.lr,\"------------------\\n\")\n",
    "m_solver=model.solver\n",
    "\n",
    "print(\"\\nSolver == \",m_solver,\"\\n\")\n",
    "model.fit(X,y,det=True)\n",
    "#_fit_gd(model,X,y)\n",
    "print('w =',model.w,'\\n\\nTrain MSE: %f'% mse(model,X,y),'\\nVal MSE: %f'% mse(model,X_val,y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Resposta - Exercício 3.1.7 --\n",
    "\n",
    "A medida que a taxa de aprendizado é reduzida, o erro de validação tende a não se alterar, usando `lr== 0.1` obteve-se `2002` iterações e para `lr == 1` obteve-se `197` iterações. Pode-se interpretar que a taxa de aprendizado mais influi no tempo que o modelo toma para convergir do que no desempenho do modelo em si.\n",
    "\n",
    "---------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Modifique a função `_fit_ne` para calcular também a matriz hessiana da função custo (regularizada), guardando-a na variável `self.hessian`. Em seguida, após o treinamento usando a solução analítica, estime o grau de condicionamento da hessiana utilizana a função `np.linalg.cond()`.\n",
    "1. Complete a função `_fit_gd` implementando o método do gradiente. Utilize os parâmetros `self.lr` (taxa de aprendizado), `self.maxiter` (número máximo de iterações) e `self.tol` (critério de parada para a norma do gradiente), e assuma como ponto inicial $\\bw = (0,\\ldots,0)$. Além de calcular `self.w`, sua função deve criar também uma lista, `self.J_history`, contendo os valores da função custo (regularizada) a cada iteração, a qual será usada para monitorar o treinamento e analisar a taxa de aprendizado.\n",
    "1. Treine o modelo sem regularização usando `solver='gd'`, trace o gráfico de `J_history` e escolha uma boa taxa de aprendizado. Quantas iterações foram necessárias para convergência?\n",
    "1. Calcule o MSE de treinamento e de validação e compare-os com os obtidos pela solução analítica. Compare também os vetores $\\bw$ das duas soluções. (Obs: a saída da célula 5 está mostrada apenas para ilustração. Não é necessário reproduzir exatamente o mesmo texto/gráfico.)\n",
    "1. (OPCIONAL) O que acontece com o erro de validação à medida que a taxa de aprendizado é reduzida? Como podemos interpretar esse fenômeno?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adicionando atributos\n",
    "\n",
    "1. Adicione atributos polinomiais de grau `d=2` usando o transformador `PolynomialFeatures`. Em seguida, repita o treinamento via solução analítica e estimação do grau de condicionamento da hessiana.\n",
    "1. Repita o treinamento usando método do gradiente (incluindo gráfico da função custo) e verifique a dificuldade de convergência. Por que isso ocorre? Foi necessário alterar a taxa de aprendizado? E o número máximo de iterações?\n",
    "1. Assim como anteriormente, compare o MSE e o $\\bw$ obtidos com os da solução analítica.\n",
    "1. Repita os itens anteriores para `d=3`.\n",
    "\n",
    "#### Dica\n",
    "\n",
    "- Não há necessidade de incluir o termo constante nos atributos adicionados, uma vez que o modelo de regressão linear já implementa a adição de coluna de 1's. Assim, utilize `PolynomialFeatures(d, include_bias=False)`.\n",
    "- Normalmente, é conveniente utilizar a função `make_pipeline` para combinar pré-processamento (transformação de atributos) e modelo de aprendizado (estimador) em um único modelo. Além de deixar o código mais compacto, essa metodologia ajuda a evitar erros de vazamento de informação entre teste e treinamento, pois garante que o transformador será treinado somente com os dados de treinamento. No entanto, como o nosso foco aqui é o treinamento, é mais conveniente primeiramente aplicar a transformação de atributos explicitamente no conjunto de dados, obtendo um conjunto transformado (aqui com sufixo `_new`), o qual é então entregue ao modelo de aprendizado. Embora não seja o caso aqui, essa abordagem também é mais eficiente quando o pré-processamento é particularmente complexo e serão realizados múltiplos treinamentos, assim o pré-processamento só precisa ser realizado uma vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Resposta - Exercício 3.2.1 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.linalg.cond(self.hessian) == \n",
      "  [[           inf 2.67140692e+16]\n",
      " [1.53694463e+16 6.06425584e+16]] \n",
      "\n",
      "w ==  [-0.35250239  0.26848578 -0.06441263]\n",
      "Train MSE: 0.511996\n",
      "Val MSE: 0.611803\n"
     ]
    }
   ],
   "source": [
    "# Feature transformation\n",
    "d = 2\n",
    "prep = PolynomialFeatures(d,include_bias=False)\n",
    "X_new = prep.fit_transform(X)\n",
    "X_val_new = prep.fit_transform(X_val)\n",
    "\n",
    "# Normal equation\n",
    "model = Model()\n",
    "model.fit(X_new, y)\n",
    "modelw=model.w\n",
    "print('w == ',model.w)\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "3\n",
      "\n",
      "--------------- \n",
      "Under tolerance \n",
      "\n",
      " \n",
      " # de iterações == 8197 \n",
      "\n",
      " \n",
      "w == [[-0.35244334]\n",
      " [ 0.26841023]\n",
      " [-0.06439475]] \n",
      "\n",
      " \n",
      "gradJ == [[ 6.05295019e-06]\n",
      " [-7.74432287e-06]\n",
      " [ 1.83311390e-06]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.511996 \n",
      "\n",
      " \n",
      "#J == 0.511996 \n",
      "\n",
      " \n",
      "---------------\n",
      "w ==  [[-0.35244334]\n",
      " [ 0.26841023]\n",
      " [-0.06439475]]\n",
      "Train MSE: 0.511996\n",
      "Val MSE: 0.611798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.35250239,  0.26848578, -0.06441263])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent -------------------\n",
    "d = 2\n",
    "prep = PolynomialFeatures(d,include_bias=False)\n",
    "X_new = prep.fit_transform(X)\n",
    "X_val_new = prep.fit_transform(X_val)\n",
    "\n",
    "model = Model(solver='gd',lr=1,maxiter=1000000)\n",
    "model.fit(X_new, y, det = True)\n",
    "\n",
    "print('w == ',model.w)\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))\n",
    "modelw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Resposta - Exercício 3.2.2 --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houve maior dificuldade de convergência por conta do aumento do grau do polinômio. Manteve-se o `learning rate == 1` e  o máximo de iterações precisou ser alterado para `maxiter == 1000000`, mas o modelo convergiu antes disso com `8197 iterações`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Resposta - Exercício 3.2.3 --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores obtidos de `w` e de `MSE` foram bem próximos da solução analítica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ~~  Exercício 3.2.4 - `d == 3` --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ - - -- - - - - -- - - - -- - - - - -- - - - -- - - - - -- - - - -- - - - - -- -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from numpy.linalg import det\n",
    "\n",
    "np.random.seed(2019*2)\n",
    "X, y = gen_data(n_samples=30, x_scale=[0,4])\n",
    "X_val, y_val = gen_data(n_samples=1000, x_scale=[0,4])\n",
    "X_test, y_test = gen_data(n_samples=1000, x_scale=[0,4])\n",
    "\n",
    "\n",
    "d = 2\n",
    "prep = make_pipeline(PolynomialFeatures(d,include_bias = False),MyStandardScaler())\n",
    "\n",
    "#Feature transformation\n",
    "nX = prep.fit_transform(X)\n",
    "nX_val =prep.fit_transform(X_val)\n",
    "\n",
    "prep.fit(nX)\n",
    "X_new = prep.transform(nX)\n",
    "X_val_new = prep.transform(nX_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 - (3.2.1 refeito com Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.linalg.cond(self.hessian) == \n",
      "  [[           inf 2.99216908e+16]\n",
      " [1.41341162e+16 6.60642650e+16]] \n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-90eb1c18f7c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Normal equation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodelw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w == '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0001dea4f350>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, det)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ne'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_ne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown solver'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0001dea4f350>\u001b[0m in \u001b[0;36m_fit_ne\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"np.linalg.cond(self.hessian) == \\n \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mlamb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Singular matrix'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Normal equation\n",
    "model = Model()\n",
    "model.fit(X_new, y)\n",
    "modelw=model.w\n",
    "print('w == ',model.w)\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 - (3.2.2 refeito com Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n",
      "6\n",
      "\n",
      "# de iterações ==    100000 \n",
      "\n",
      " \n",
      "w == [[-0.11334276]\n",
      " [-0.02909321]\n",
      " [-0.02118497]\n",
      " [-0.02118497]\n",
      " [-0.00379373]\n",
      " [ 0.013593  ]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.03675658]\n",
      " [-0.00801463]\n",
      " [-0.00298954]\n",
      " [-0.00298954]\n",
      " [-0.01389869]\n",
      " [-0.02739223]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.518956 \n",
      "\n",
      " \n",
      "#J == 0.518956 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    200000 \n",
      "\n",
      " \n",
      "w == [[-0.13761222]\n",
      " [-0.01898189]\n",
      " [-0.01845287]\n",
      " [-0.01845287]\n",
      " [ 0.00903911]\n",
      " [ 0.03959178]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.01704019]\n",
      " [-0.00992852]\n",
      " [-0.00109283]\n",
      " [-0.00109283]\n",
      " [-0.01080834]\n",
      " [-0.02389001]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.517376 \n",
      "\n",
      " \n",
      "#J == 0.517376 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    300000 \n",
      "\n",
      " \n",
      "w == [[-0.15093473]\n",
      " [-0.01006905]\n",
      " [-0.01919771]\n",
      " [-0.01919771]\n",
      " [ 0.01785817]\n",
      " [ 0.06151361]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.01026979]\n",
      " [-0.00798167]\n",
      " [ 0.00236549]\n",
      " [ 0.00236549]\n",
      " [-0.00707386]\n",
      " [-0.02019764]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.516551 \n",
      "\n",
      " \n",
      "#J == 0.516551 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    400000 \n",
      "\n",
      " \n",
      "w == [[-0.15903476]\n",
      " [-0.00278436]\n",
      " [-0.02276731]\n",
      " [-0.02276731]\n",
      " [ 0.02363078]\n",
      " [ 0.08042084]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.00625167]\n",
      " [-0.00669446]\n",
      " [ 0.00459939]\n",
      " [ 0.00459939]\n",
      " [-0.00465419]\n",
      " [-0.01779422]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.516013 \n",
      "\n",
      " \n",
      "#J == 0.516013 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    500000 \n",
      "\n",
      " \n",
      "w == [[-0.16394256]\n",
      " [ 0.00349289]\n",
      " [-0.0281169 ]\n",
      " [-0.0281169 ]\n",
      " [ 0.02745924]\n",
      " [ 0.0973858 ]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.0037613 ]\n",
      " [-0.00592848]\n",
      " [ 0.00598951]\n",
      " [ 0.00598951]\n",
      " [-0.00311814]\n",
      " [-0.01624759]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.515588 \n",
      "\n",
      " \n",
      "#J == 0.515588 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    600000 \n",
      "\n",
      " \n",
      "w == [[-0.16687075]\n",
      " [ 0.00918129]\n",
      " [-0.03457101]\n",
      " [-0.03457101]\n",
      " [ 0.03005065]\n",
      " [ 0.11309409]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.00221823]\n",
      " [-0.00549097]\n",
      " [ 0.00684979]\n",
      " [ 0.00684979]\n",
      " [-0.00213692]\n",
      " [-0.01523909]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.515210 \n",
      "\n",
      " \n",
      "#J == 0.515210 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    700000 \n",
      "\n",
      " \n",
      "w == [[-0.16857465]\n",
      " [ 0.01454242]\n",
      " [-0.04170697]\n",
      " [-0.04170697]\n",
      " [ 0.03184799]\n",
      " [ 0.12797557]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.00126644]\n",
      " [-0.00525774]\n",
      " [ 0.0073791 ]\n",
      " [ 0.0073791 ]\n",
      " [-0.00150297]\n",
      " [-0.01456783]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.514852 \n",
      "\n",
      " \n",
      "#J == 0.514852 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    800000 \n",
      "\n",
      " \n",
      "w == [[-0.16952575]\n",
      " [ 0.01973864]\n",
      " [-0.04926083]\n",
      " [-0.04926083]\n",
      " [ 0.03312857]\n",
      " [ 0.1422998 ]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.00068374]\n",
      " [-0.00515107]\n",
      " [ 0.00770176]\n",
      " [ 0.00770176]\n",
      " [-0.00108653]\n",
      " [-0.01410824]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.514503 \n",
      "\n",
      " \n",
      "#J == 0.514503 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    900000 \n",
      "\n",
      " \n",
      "w == [[-0.17001833]\n",
      " [ 0.02487036]\n",
      " [-0.05706783]\n",
      " [-0.05706783]\n",
      " [ 0.03406619]\n",
      " [ 0.15623612]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.00033134]\n",
      " [-0.00512242]\n",
      " [ 0.00789547]\n",
      " [ 0.00789547]\n",
      " [-0.00080651]\n",
      " [-0.01378182]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.514159 \n",
      "\n",
      " \n",
      "#J == 0.514159 \n",
      "\n",
      " \n",
      "---------------\n",
      "\n",
      "# de iterações ==    1000000 \n",
      "\n",
      " \n",
      "w == [[-0.17023593]\n",
      " [ 0.02999927]\n",
      " [-0.06502517]\n",
      " [-0.06502517]\n",
      " [ 0.03476996]\n",
      " [ 0.16989127]] \n",
      "\n",
      " \n",
      "gradJ == [[ 0.00012251]\n",
      " [-0.00514151]\n",
      " [ 0.00800875]\n",
      " [ 0.00800875]\n",
      " [-0.00061222]\n",
      " [-0.01353951]] \n",
      "\n",
      " \n",
      "|gradJ| == 0.513819 \n",
      "\n",
      " \n",
      "#J == 0.513819 \n",
      "\n",
      " \n",
      "---------------\n",
      "Reached maxiter\n",
      "w ==  [[-0.17023593]\n",
      " [ 0.02999927]\n",
      " [-0.06502517]\n",
      " [-0.06502517]\n",
      " [ 0.03476996]\n",
      " [ 0.16989127]]\n",
      "Train MSE: 0.513819\n",
      "Val MSE: 0.593860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.35250239,  0.26848578, -0.06441263])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent -------------------\n",
    "d = 2\n",
    "\n",
    "model = Model(solver='gd',lr=1,maxiter=1000000)\n",
    "model.fit(X_new, y, det = True)\n",
    "\n",
    "print('w == ',model.w)\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))\n",
    "modelw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 - (3.2.3 refeito com Scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 - (3.2.4 refeito com Scaler) 1 - Solução analítica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 - (3.2.4 refeito com Scaler) 2 e 3 - Método do gradiente & comparação do `MSE` e `w`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 - resposta (3.2.4 refeito com Scaler) 2 e 3 - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ - - -- - - - - -- - - - -- - - - - -- - - - -- - - - - -- - - - -- - - - - -- -\n",
    "z\n",
    "\n",
    "z\n",
    "\n",
    "z\n",
    "\n",
    "z\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 - 1 - Solução analítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation\n",
    "d = 3\n",
    "prep = PolynomialFeatures(d,include_bias=False)\n",
    "X_new = prep.fit_transform(X)\n",
    "X_val_new = prep.fit_transform(X_val)\n",
    "\n",
    "# Normal equation\n",
    "model = Model()\n",
    "model.fit(X_new, y)\n",
    "modelw=model.w\n",
    "print('w == ',model.w)\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 - 2 e 3 - Método do gradiente & comparação do `MSE` e `w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent -------------------\n",
    "d = 3\n",
    "print(\"modelw == \",modelw)\n",
    "prep = PolynomialFeatures(d,include_bias=False)\n",
    "X_new = prep.fit_transform(X)\n",
    "X_val_new = prep.fit_transform(X_val)\n",
    "\n",
    "# Normal equation\n",
    "model = Model(solver='gd',lr=1,maxiter=1000000)\n",
    "model.fit(X_new, y, det = True)\n",
    "\n",
    "print('w == ',model.w)\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))\n",
    "modelw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta  - 3.2.4 - (2 e 3) \n",
    "O número máximo de iterações teve de ser aumentado para `it == 1000000`, mas não chegou a haver convergência para `d == 3` com um `J` satisfatório. \n",
    "\n",
    "#### ---- Para `lr == 1`, ----------------------------------------\n",
    "O modelo convergiu, com `554776 iterações`, porém obteve-se um valor baiixo de `MSE`. \n",
    "##### `MSE`\n",
    "O modelo então converge com `Train MSE == 0.325498` e `Val MSE == 0.334952` para `554776 iterações`.\n",
    "\n",
    "##### `w`\n",
    "o valor de `w` começa a convergir para o obtido na solução analítica já em `400000 iterações` e já fica bem próximo quando são feitas `500000 iterações`.\n",
    "\n",
    "#### ---- Alterando a taxa de aprendizado de `lr == 1` para `lr == 0.001` ----------------------------------------\n",
    "\n",
    "##### `MSE`\n",
    "Obteve-se `train MSE == 0,5084` e `Val MSE == 0,5680` e o modelo não convergiu quando já havia passado das `1000000 iterações`; estas previamente estipuladas em `maxiter`.\n",
    "##### `w`\n",
    "Os valores de `w` tornaram-se bem menores e se distanciaram do valor obtido na solução analítica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Escalonamento de atributos\n",
    "\n",
    "Implemente a normalização (escalonamento) de atributos conforme vista em sala, a qual consiste de:\n",
    " - Subtração da média do atributo, para que passe a ter média nula\n",
    " - Divisão pelo desvio padrão do atributo, para que passe a ter variância unitária\n",
    " \n",
    "Esse tipo de normalização também é chamado em alguns contextos de padronização (*standardization*), no sentido de resultar na mesma média (0) e variância (1) de uma variável aleatória gaussiana padrão (*standard*), em contraste com outros tipos de normalização (por exemplo, reescalonamento para a escala [0,1]).\n",
    "\n",
    "1. Para isso, complete a classe abaixo. Caso deseje confirmar se sua implementação está correta, compare com o transformador `StandardScaler` do módulo `sklearn.preprocessing`.\n",
    "1. Após implementar corretamente, verifique que seu escalonador funciona corretamente em um pipeline do `sklearn`; isto é, combine todas as etapas de pré-processamento (transformação de atributos e escalonamento) usando `make_pipeline`. Em seguida, você pode ignorar sua implementação e passar a usar o `StandardScaler`.\n",
    "1. Refaça os mesmos passos da seção anterior (2.1-2.4) e compare os resultados e o comportamento do algoritmo. Explique.\n",
    "1. Neste problema, em qual posição o escalonador funciona melhor, antes ou depois da adição de atributos? Cite as evidências que você observou.\n",
    "1. O uso do escalonador tem impacto do desempenho da solução analítica? Por quê?\n",
    "1. (OPCIONAL) Experimente outros escalonadores do `sklearn`, como `MinMaxScaler` e `MaxAbsScaler`, e compare o desempenho obtido.\n",
    "\n",
    "#### Dicas\n",
    "\n",
    "- Funções úteis:\n",
    "\n",
    "```python\n",
    "np.mean(axis=0), np.std(axis=0)\n",
    "```\n",
    "\n",
    "- Revise as propriedades de broadcasting do NumPy, em particular em operações envolvendo matrizes e arrays 1D.\n",
    "- Para depurar possíveis erros, lembre-se de verificar o `shape` dos arrays envolvidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Resposta - Exercício 3.3.1 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "class MyStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute and store scaler parameters\n",
    "        self.mean = np.mean(X, axis = 0)\n",
    "        self.std = np.std(X, axis = 0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Scale features\n",
    "        X_new = X\n",
    "        X_new = X_new.astype(float)\n",
    "        mean = self.mean\n",
    "        std = self.std\n",
    "        for col in range(1,X.shape[1]):\n",
    "            X_new[:,col] = (X_new[:,col] - mean[col])/std[col]\n",
    "        return X_new\n",
    "\n",
    "A = np.array([[1, 1, 1], [2, 4, 8], [3, 9, 27], [4, 16, 64]])\n",
    "\n",
    "m = make_pipeline(MyStandardScaler())\n",
    "m.fit(A)\n",
    "nA = m.transform(A)\n",
    "print(\"na (MyStandardScaler):\\n\",nA)\n",
    "\n",
    "mm = make_pipeline(StandardScaler())\n",
    "mm.fit(A)\n",
    "nnA = m.transform(A)\n",
    "print(\"\\nnna (Standard Scaler do Sklearn):\\n\",nnA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Resposta - Exercício 3.3.2 --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Usando MyStandardScaler() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = 2\n",
    "prep = make_pipeline(MyStandardScaler(),PolynomialFeatures(d,include_bias = False))\n",
    "\n",
    "#Feature transformation\n",
    "nX = prep.fit_transform(X)\n",
    "nX_val =prep.fit_transform(X_val)\n",
    "\n",
    "m.fit(nX)\n",
    "X_new = m.transform(nX)\n",
    "X_val_new = m.transform(nX_val)\n",
    "\n",
    "#Gradient Descent\n",
    "model = Model(solver = 'gd',lr = 12, maxiter = 10000)\n",
    "model.fit(X_new,y,det = True)\n",
    "\n",
    "print('w = {}'.format(model.w))\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Usando StandardScaler() do Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = 2\n",
    "prep = make_pipeline(StandardScaler(),PolynomialFeatures(d,include_bias = False))\n",
    "\n",
    "#Feature transformation\n",
    "nX = prep.fit_transform(X)\n",
    "nX_val =prep.fit_transform(X_val)\n",
    "\n",
    "m.fit(nX)\n",
    "X_new = m.transform(nX)\n",
    "X_val_new = m.transform(nX_val)\n",
    "\n",
    "#Gradient Descent\n",
    "model = Model(solver = 'gd',lr = 12, maxiter = 10000)\n",
    "model.fit(X_new,y,det = True)\n",
    "\n",
    "print('w = {}'.format(model.w))\n",
    "print('Train MSE: %f' % mse(model,X_new,y))\n",
    "print('Val MSE: %f' % mse(model,X_val_new,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----  Apesar de os valores de `w` obtidos pelo pipeline da função personalizada terem sido diferentes, os valores de `J`, de `Train MSE` e `Val MSE` foram compatíveis com os valores obtidos pelo pipeline que usou a função `StandardScaler` do Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Resposta - Exercício 3.3.4 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ainda mais atributos\n",
    "\n",
    "1. Adicione atributos polinomiais de grau ainda maior (ex: d=4, d=5). O que você observa?\n",
    "1. Você recomendaria o método do gradiente para um problema desse tipo? Ou seria melhor usar um método de segunda ordem? Explique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conjunto de dados #2\n",
    "\n",
    "O segundo conjunto de dados que usaremos consiste de dados sobre a venda de casas em King County, USA, entre maio de 2014 e maio de 2015. O objetivo é prever o preço de venda a partir de informações sobre a casa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Original source: http://www.kaggle.com/harlfoxem/housesalesprediction/data\n",
    "df = pd.read_csv('https://github.com/danilo-silva-ufsc/ml/raw/master/data/kc_house_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como variável de saída, $y$, utilize o **logaritmo** neperiano do preço de venda , `price`, i.e., `np.log(price)`. Desta forma o erro na predição de $y$ será função do erro _relativo_ na predição do preço, evitando dar peso excessivo aos preços mais altos. Por exemplo, quando a função perda é o erro quadrático, a perda equivale aproximadamente ao quadrado do erro relativo:\n",
    "\n",
    "$L(y,\\hat{y}) = (\\hat{y} - y)^2 = (\\log(\\hat{p}) - \\log(p))^2 = (\\log(\\hat{p}/p))^2 = (\\log(1 + (\\hat{p}-p)/p))^2 \\approx ((\\hat{p}-p)/p)^2$\n",
    "\n",
    "\n",
    "Como atributos, utilize apenas as 4 colunas após o preço de venda, i.e.:\n",
    "- `bedrooms`: número de quartos\n",
    "- `bathrooms`: número de quartos, em múltiplos de 1/4 (https://www.realtor.com/advice/buy/what-is-a-half-bath/)\n",
    "- `sqft_living`: área da casa, em ft${^2}$\n",
    "- `sqft_lot`: área do lote, em ft${^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação\n",
    "\n",
    "1. Prepare e divida o conjunto de dados aleatoriamente em conjuntos de treinamento, validação e teste, nas proporções 60%, 20% e 20%, respectivamente. Para isso, utilize a função `sklearn.model_selection.train_test_split()`.\n",
    "1. Como função perda para o treinamento, utilize o erro quadrático, e, como métrica de avaliação do modelo, utilize a raiz quadrada do erro quadrático médio. Ambos são equivalentes, mas o segundo resulta em valores numa escala mais agradável para análise e mais fácil de interpretar. Adicionalmente, utilize como métrica de avaliação o [erro percentual absoluto médio](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) (MAPE) do preço de venda (i.e., da variável original `price`, **não** da variável `y = np.log(price)`). Esta métrica é ainda mais fácil de interpretar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "df = df[df['bedrooms'] < 10]\n",
    "df = df[df['bathrooms'] < 6]\n",
    "df = df[df['sqft_living'] < 7000]\n",
    "df = df[df['sqft_lot'] < 600e3]\n",
    "\n",
    "X = df[['bedrooms','bathrooms','sqft_living','sqft_lot']].to_numpy()\n",
    "y = ???\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(???, test_size=y_test.shape[0], random_state=0)\n",
    "del(X,y) # just to make sure we will not use them by mistake. Or set X,y = X_train,y_train\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(model, X, y):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(model, X, y):\n",
    "    p = ???\n",
    "    p_pred = ???\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploração dos dados\n",
    "\n",
    "Antes de escolher e começar a treinar um modelo, é útil fazer uma breve exploração dos dados. (Foi dessa exploração inicial que surgiu a ideia, por exemplo, de remover outliers, com aqueles valores específicos.) \n",
    "\n",
    "3. Para cada atributo, trace o gráfico da variável de saída em função do atributo, **sobre o conjunto de treinamento** (não trace gráficos sobre o conjunto de teste para evitar vazamento de informação). Observe as escalas das variáveis envolvidas e analise se há alguma dependência aparente entre as variáveis. Intuitivamente, qual atributo parece ser mais preditivo do preço do venda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regressão linear\n",
    "\n",
    "4. Inicialmente você deve treinar um modelo de regressão linear sem regularização e calcular o desempenho da predição (RMSE e MAPE) sobre o conjunto de treinamento e sobre o conjunto de validação. Fique à vontade para usar as funções do `sklearn`, não há necessidade de usar o método do gradiente.\n",
    "1. Você diria que o modelo treinado sofre de underfitting, overfitting ou nenhum dos dois? Explique.\n",
    "1. Analisando o vetor de pesos do modelo treinado (`model.coef_`), qual atributo você diria que é o mais importante para a predição? Por quê? Esta observação confirma a sua hipótese do item anterior? Explique.\n",
    "\n",
    "#### Dica\n",
    "- Para acessar o regressor dentro de um *pipeline* do sklearn, inicialize-o fora do *pipeline* ou acesse-o via `model.steps[-1][1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aprimorando o modelo\n",
    "\n",
    "7. Usando o que vimos até agora na disciplina, tente ao máximo melhorar o desempenho do modelo neste conjunto de dados. Reporte o desempenho obtido (RMSE e MAPE).\n",
    "\n",
    "#### Dica:\n",
    "- Reveja os conceitos aprendidos na Aula 2 e no Exercício 2.\n",
    "- Se desejar aplicar alguma transformação de atributos \"customizada\", você tem duas opções: criar um transformador customizado do `sklearn` e integrá-lo em uma *pipeline* (ver último item opcional do Exercício 2), ou, *somente se for uma transformação que não envolve estimação de parâmetros*, você pode aplicá-la diretamente a todo o conjunto de dados (matrix $\\bX$ antes do *split*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (OPCIONAL)\n",
    "\n",
    "- Tente utilizar mais colunas da tabela original para melhorar o desempenho.\n",
    "- Utilize um outro conjunto de dados com múltiplos atributos. Sugestão: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
